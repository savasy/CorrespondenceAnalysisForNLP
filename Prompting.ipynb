{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prompting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNP5tSV8I6zEUkiGazT+AY6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/savasy/CorrespondenceAnalysisForNLP/blob/main/Prompting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt-based learning"
      ],
      "metadata": {
        "id": "EGSXXopgD17q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt-based learning is a new paradigm in the NLP field. In prompt-based learning, we do not have to hold any supervised learning process since we directly rely on the objective function (such as MLM) of any pre-trained language model. In order to use the models to achieve prediction tasks, the only thing to be done is to modify the original input<X> using a task-specific template into a textual string prompt such as <X, that is [MASK]> so that the model can achieve the task even without learning.\n",
        "Such a mechanism allows us to exploit the LM that is pre-trained on huge amounts of textual data. This prompting function can be defined to make any LM be able to achieve few-shot, one-shot, or even zero-shot learning tasks where we easily adapt the model to new scenarios even with few or no labeled data."
      ],
      "metadata": {
        "id": "7ceJT8xK0FQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets"
      ],
      "metadata": {
        "id": "RrihLWeJHGGt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e745f1d-e423-4079-9935-bf113879db59"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.5 MB 4.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 311 kB 59.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 48.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 51.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 32.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 243 kB 43.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 46.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 56.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 49.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 57.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/MyDrive/akademi/PromptingDeneme\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbijpKzDPou9",
        "outputId": "deb2d6d8-73cf-4055-8e77-e71ed04a0baa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7ZJUuOvJP1FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM , AutoTokenizer\n",
        "import torch\n",
        "model_path=\"dbmdz/bert-base-turkish-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "Oi--laJuLvEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting Class\n",
        "Here is the class definition for Prompting"
      ],
      "metadata": {
        "id": "gY9tUJ2WEJQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM , AutoTokenizer\n",
        "class Prompting(object):\n",
        "  \"\"\" doc string \n",
        "   This class helps us to implement\n",
        "   Prompt-based Learning Model\n",
        "  \"\"\"\n",
        "  def __init__(self, **kwargs):\n",
        "    \"\"\" constructor \n",
        "\n",
        "    parameter:\n",
        "    ----------\n",
        "       model: AutoModelForMaskedLM\n",
        "            path to a Pre-trained language model form HuggingFace Hub\n",
        "       tokenizer: AutoTokenizer\n",
        "            path to tokenizer if different tokenizer is used, \n",
        "            otherwise leave it empty\n",
        "    \"\"\"\n",
        "    model_path=kwargs['model']\n",
        "    tokenizer_path= kwargs['model']\n",
        "    if \"tokenizer\" in kwargs.keys():\n",
        "      tokenizer_path= kwargs['tokenizer']\n",
        "    self.model = AutoModelForMaskedLM.from_pretrained(model_path)\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "  def prompt_pred(self,text):\n",
        "    \"\"\"\n",
        "      Predict MASK token by listing the probability of candidate tokens \n",
        "      where the first token is the most likely\n",
        "\n",
        "      Parameters:\n",
        "      ----------\n",
        "      text: str \n",
        "          The text including [MASK] token.\n",
        "          It supports single MASK token. If more [MASK]ed tokens \n",
        "          are given, it takes the first one.\n",
        "\n",
        "      Returns:\n",
        "      --------\n",
        "      list of (token, prob)\n",
        "         The return is a list of all token in LM Vocab along with \n",
        "         their prob score, sort by score in descending order \n",
        "    \"\"\"\n",
        "    indexed_tokens=tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "    tokenized_text= tokenizer.convert_ids_to_tokens (indexed_tokens[0])\n",
        "    # take the first masked token\n",
        "    mask_pos=tokenized_text.index(tokenizer.mask_token)\n",
        "    self.model.eval()\n",
        "    with torch.no_grad():\n",
        "      outputs = model(indexed_tokens)\n",
        "      predictions = outputs[0]\n",
        "    values, indices=torch.sort(predictions[0, mask_pos],  descending=True)\n",
        "    #values=torch.nn.functional.softmax(values, dim=0)\n",
        "    result=list(zip(self.tokenizer.convert_ids_to_tokens(indices), values))\n",
        "    self.scores_dict={a:b for a,b in result}\n",
        "    return result\n",
        "\n",
        "  def compute_tokens_prob(self, text, token_list1, token_list2):\n",
        "    \"\"\"\n",
        "    Compute the activations for given two token list, \n",
        "\n",
        "    Parameters:\n",
        "    ---------\n",
        "    token_list1: List(str)\n",
        "     it is a list for positive polarity tokens such as good, great. \n",
        "    token_list2: List(str)\n",
        "     it is a list for negative polarity tokens such as bad, terrible.      \n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Tuple (\n",
        "       the probability for first token list,\n",
        "       the probability of the second token list,\n",
        "       the ratio score1/ (score1+score2)\n",
        "       The softmax returns\n",
        "    \"\"\"\n",
        "    _=self.prompt_pred(text)\n",
        "    score1=[self.scores_dict[token1] if token1 in self.scores_dict.keys() else 0\\\n",
        "            for token1 in token_list1]\n",
        "    score1= sum(score1)\n",
        "    score2=[self.scores_dict[token2] if token2 in self.scores_dict.keys() else 0\\\n",
        "            for token2 in token_list2]\n",
        "    score2= sum(score2)\n",
        "    softmax_rt=torch.nn.functional.softmax(torch.Tensor([score1,score2]), dim=0)\n",
        "    return score1, score2, score1/ (score1+score2),softmax_rt\n",
        "\n",
        "  def fine_tune(self, sentences, labels, prompt=\" Çünkü [MASK] idi.\",goodToken=\"iyi\",badToken=\"kötü\"):\n",
        "    \"\"\"  \n",
        "      Fine tune the model\n",
        "    \"\"\"\n",
        "    good=tokenizer.convert_tokens_to_ids(goodToken)\n",
        "    bad=tokenizer.convert_tokens_to_ids(badToken)\n",
        "\n",
        "    from transformers import AdamW\n",
        "    optimizer = AdamW(self.model.parameters(),lr=1e-3)\n",
        "\n",
        "    for sen, label in zip(sentences, labels):\n",
        "      tokenized_text = self.tokenizer.tokenize(sen+prompt)\n",
        "      indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "      tokens_tensor = torch.tensor([indexed_tokens])\n",
        "      # take the first masked token\n",
        "      mask_pos=tokenized_text.index(\"[MASK]\")\n",
        "      outputs = self.model(tokens_tensor)\n",
        "      predictions = outputs[0]\n",
        "      pred=predictions[0, mask_pos][[good,bad]]\n",
        "      prob=torch.nn.functional.softmax(pred, dim=0)\n",
        "      lossFunc = torch.nn.CrossEntropyLoss()\n",
        "      loss=lossFunc(prob.unsqueeze(0), torch.tensor([label]))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    print(\"done!\")\n"
      ],
      "metadata": {
        "id": "oW_TsjAlc7Q2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I take Turkish LM here, you can choose any other model. "
      ],
      "metadata": {
        "id": "5e4LgLhmFIR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompting= Prompting(model=\"dbmdz/bert-base-turkish-cased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T698UBNks760",
        "outputId": "791d7c3d-c350-4108-a90f-62f450a0a816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-base-turkish-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let the model predict some tokens"
      ],
      "metadata": {
        "id": "GWBBUoqCFUiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A POSITIVE example Input"
      ],
      "metadata": {
        "id": "v-llHxmqhhR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Çok keyif aldım filmden\" # Which means: I liked the film,\n",
        "prompt=\". çünkü [MASK] idi.\" #  since it was [MASK]\n",
        "prompted= text + prompt\n",
        "prompting.prompt_pred(prompted)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1n23xEqxDgA9",
        "outputId": "40e0bdff-a643-4f94-b5cb-ab446658b130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('güzel', tensor(7.8681)),\n",
              " ('eski', tensor(7.6136)),\n",
              " ('harika', tensor(7.5754)),\n",
              " ('mükemmel', tensor(7.5497)),\n",
              " ('eğlenceli', tensor(7.5261)),\n",
              " ('yeni', tensor(7.5034)),\n",
              " ('muhteşem', tensor(7.3991)),\n",
              " ('kötü', tensor(7.3724)),\n",
              " ('komik', tensor(7.3256)),\n",
              " ('iyi', tensor(7.3105))]"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gmaTfYHdhfzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A NEGATIVE example Input"
      ],
      "metadata": {
        "id": "2OY531QMhl4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Çok keyif almadım filmden\" # Which means: I didn't enjoy the film\n",
        "propmt=\". çünkü [MASK] idi.\" # , since it was [MASK]\n",
        "prompted= text + propmt\n",
        "prompting.prompt_pred(prompted)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWEQ2sE6hga-",
        "outputId": "0faa2fc4-333d-4d64-c79b-c0e2834630c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('kötü', tensor(8.0674)),\n",
              " ('eski', tensor(7.9916)),\n",
              " ('berbat', tensor(7.5965)),\n",
              " ('yeni', tensor(7.5006)),\n",
              " ('gereksiz', tensor(7.4864)),\n",
              " ('iğrenç', tensor(7.3922)),\n",
              " ('sıkıcı', tensor(7.2912)),\n",
              " ('korkunç', tensor(7.2623)),\n",
              " ('saçma', tensor(7.2231)),\n",
              " ('güzel', tensor(7.1390))]"
            ]
          },
          "metadata": {},
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Producing the results for  a pair of neg/pos words\n",
        "Now we pass a list of neg/pos words rather thansinlge neg/pos words (tokens)"
      ],
      "metadata": {
        "id": "Y56NnjdNFZJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Çok keyif almadım filmden\"\n",
        "propmt=\", çünkü [MASK] idi.\"\n",
        "prompted= text + propmt\n",
        "prompting.compute_tokens_prob(prompted, [\"gereksiz\"], [\"harika\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og1uFMJVtq5E",
        "outputId": "cef430b6-a413-43e8-a47a-22ef0bcfa12c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(8.3082), tensor(7.0045), tensor(0.5426), tensor([0.7865, 0.2135]))"
            ]
          },
          "metadata": {},
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Çok keyif almadım filmden\"\n",
        "propmt=\", çünkü [MASK] idi.\"\n",
        "prompted= text + propmt\n",
        "prompting.compute_tokens_prob(prompted, [\"gereksiz\",\"kötü\", \"berbat\",\"sıkıcı\"], [\"harika\",\"güzel\",\"mükemmel\",\"muhteşem\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI1fQge0d5Mn",
        "outputId": "bd82c8ea-d4b1-4f1c-d792-8ab3e9d44a42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(32.1539), tensor(28.4894), tensor(0.5302), tensor([0.9750, 0.0250]))"
            ]
          },
          "metadata": {},
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Çok keyif aldım filmden\"\n",
        "prompted= text + propmt\n",
        "prompting.compute_tokens_prob(prompted, [\"gereksiz\"], [\"harika\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRM6AH_FDKsC",
        "outputId": "192a87c0-60eb-424a-d38d-76de72938d9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(6.8251), tensor(8.9176), tensor(0.4335), tensor([0.1098, 0.8902]))"
            ]
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompting.compute_tokens_prob(prompted, [\"gereksiz\",\"kötü\", \"berbat\",\"sıkıcı\"], [\"harika\",\"güzel\",\"mükemmel\",\"muhteşem\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZauM9VsSeYDC",
        "outputId": "05ad12f8-8df7-40c4-b53a-3727410918de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(26.2229),\n",
              " tensor(34.5780),\n",
              " tensor(0.4313),\n",
              " tensor([2.3515e-04, 9.9976e-01]))"
            ]
          },
          "metadata": {},
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning"
      ],
      "metadata": {
        "id": "V-7vHaV_GG_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check LM to find the which words are the most suitable (pos or neg) in according to LM. "
      ],
      "metadata": {
        "id": "ShNmgDjh9bg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.read_csv(\"film.tsv\", sep=\"\\t\", header=None)\n",
        "df.columns=[\"text\",\"label\"]\n",
        "df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "BkfxRB-JJIj_",
        "outputId": "1c0c5b41-9063-4209-a514-fb8b614b1e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7786e942-e14d-4686-8f1e-5c39ac304d1d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aglamaktan perisan oldugum bir filmdi..</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>altan erkekli benim için artik son derece dege...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bastan asagi mantik hatalariyla dolu, yeni nes...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7786e942-e14d-4686-8f1e-5c39ac304d1d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7786e942-e14d-4686-8f1e-5c39ac304d1d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7786e942-e14d-4686-8f1e-5c39ac304d1d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                text  label\n",
              "0            aglamaktan perisan oldugum bir filmdi..      1\n",
              "1  altan erkekli benim için artik son derece dege...      0\n",
              "2  bastan asagi mantik hatalariyla dolu, yeni nes...      0"
            ]
          },
          "metadata": {},
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separate neg/pos corpus"
      ],
      "metadata": {
        "id": "a77U5mG69uGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos=df[df.label==1]\n",
        "neg=df[df.label==0]\n",
        "pos.shape, neg.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1NhH4FbM2m8",
        "outputId": "c13f2fdc-d94f-48f0-98c1-10bdc1e61919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1644, 2), (1620, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "N5McylF8NaxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take first 200 examples for pos/neg\n",
        "pos200=pos[\"text\"].values[:200]\n",
        "neg200=neg[\"text\"].values[:200]"
      ],
      "metadata": {
        "id": "2n00Hu26NjHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lets find positive tokens for a given template"
      ],
      "metadata": {
        "id": "7parHdu9UDC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tokens=[]\n",
        "prompt=\", yani [MASK] bir film.\"\n",
        "\n",
        "for i,t in enumerate(pos200[:5]):\n",
        "  if i%25==0:\n",
        "    print(i)\n",
        "  prompted= \" \".join(t.split()[:10]) + prompt\n",
        "  res=prompting.prompt_pred(prompted)[:10]\n",
        "  res2=[e[0] for e in res]\n",
        "  pos_tokens+= res2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPUAz3YDOkGJ",
        "outputId": "ccc3698c-072d-4c10-90de-ad2f2a36eefd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "cp=collections.Counter(pos_tokens)\n",
        "cp.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTsQQN8ETeWG",
        "outputId": "0c3cdd33-d753-40d7-baf0-c168742df210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('iyi', 4),\n",
              " ('güzel', 4),\n",
              " ('tek', 3),\n",
              " ('harika', 3),\n",
              " ('yeni', 2),\n",
              " ('kötü', 2),\n",
              " ('klasik', 2),\n",
              " ('ikinci', 2),\n",
              " ('özel', 2),\n",
              " ('baska', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lets find negative tokens for a given template"
      ],
      "metadata": {
        "id": "Y2fGbMGoULcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neg_tokens=[]\n",
        "for i,t in enumerate(neg200[:5]):\n",
        "  if i%25==0:\n",
        "    print(i)\n",
        "  prompted= t + prompt\n",
        "  res=prompting.prompt_pred(prompted)[:10]\n",
        "  res2=[e[0] for e in res]\n",
        "  neg_tokens+= res2"
      ],
      "metadata": {
        "id": "8ytBjT6OThTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0761dbd2-9105-43a8-c607-0017aca57aa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "cn=collections.Counter(neg_tokens)\n",
        "cn.most_common(8)"
      ],
      "metadata": {
        "id": "XRQy308GUT2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe4df542-1685-420c-a866-2a4b15ad62c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('iyi', 4),\n",
              " ('yeni', 4),\n",
              " ('kötü', 3),\n",
              " ('böyle', 3),\n",
              " ('öyle', 3),\n",
              " ('sadece', 3),\n",
              " ('##t', 3),\n",
              " ('baska', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 238
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Q_TZSFkMVwQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "XM3j7z1EwWjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_test200=pos[\"text\"][200:400]\n",
        "neg_test200=neg[\"text\"][200:400]"
      ],
      "metadata": {
        "id": "Zscz0yzCmMzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\", son derece [MASK] bir filmdi.\""
      ],
      "metadata": {
        "id": "t99G1dUKesXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For POSIVITE Set Eval."
      ],
      "metadata": {
        "id": "ZyfprMrPl8JS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_best_tokens=[\"harika\",\"mükemmel\",\"muhteşem\",\"süper\"]\n",
        "neg_best_tokens=[\"kötü\",\"berbat\",\"sıkıcı\",\"sadece\"]\n",
        "\n",
        "crr=0\n",
        "for i,t in enumerate(pos_test200):\n",
        "  if i%25==0:\n",
        "    print(\"%s. step, and the number of correct case is %s\"%(i,crr))\n",
        "  prompted= t + prompt\n",
        "  res=prompting.compute_tokens_prob(prompted, pos_best_tokens, neg_best_tokens)\n",
        "  if res[2]>0.5:\n",
        "    crr+=1\n",
        "print(crr)"
      ],
      "metadata": {
        "id": "pqm-TRzoWAMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nKW-wG1mYmOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For Negative set Eval"
      ],
      "metadata": {
        "id": "PXf0usErhCVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crr=0\n",
        "for i,t in enumerate(neg_test200):\n",
        "  if i%25==0:\n",
        "    print(\"%s. step, and the number of correct case is %s\"%(i,crr))\n",
        "  prompted= t + prompt\n",
        "  res=prompting.compute_tokens_prob(prompted, pos_best_tokens, neg_best_tokens)\n",
        "  if res[2]<0.5:\n",
        "    crr+=1\n",
        "print(crr)"
      ],
      "metadata": {
        "id": "BTeQX0bVbEyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6Q6OwT8MhBh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning"
      ],
      "metadata": {
        "id": "RhtJeOoZZ8Kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompting.fine_tune(pos200+neg200, [1]*len(pos200)+ [0]* len(neg200))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "3Z47ZuYAfQ3d",
        "outputId": "6e65f62b-9310-4007-96dd-1b326b23dfa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-242-8289f0568096>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprompting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfine_tune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos200\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mneg200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-224-672695e7336e>\u001b[0m in \u001b[0;36mfine_tune\u001b[0;34m(self, sentences, labels, prompt, goodToken, badToken)\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mlossFunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlossFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[1,3,4]+[3,4,1,1,1,1,1,1]"
      ],
      "metadata": {
        "id": "0RqNK5RWdKCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uKyFfRe1VZcg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}